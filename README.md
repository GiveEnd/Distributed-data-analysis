# **Анализ распределенных данных**
Анализ распределённых данных — это работа с данными, которые хранятся на разных узлах сети. Он включает в себя различные методы статистической обработки, которые позволяют выявлять закономерности, тенденции и связи, которые могут быть скрытыми в больших объёмах данных.
**Два подхода:**

-   **Централизованный анализ** — сбор всех данных в единое хранилище, а затем обработка.
    
-   **Федеративный анализ** — обработка данных там, где они хранятся, и объединение результатов.

## **Поколения платформ анализа данных. Хранилища данных.**

**Три поколения платформ:**

1.  **Хранилища данных (Data Warehouses)** —централизованная система для хранения обработанных структурированных данных, предназначенная в первую очередь для отчётности и BI. Данные из разных источников (CRM, ERP и др.) собираются, очищаются (ETL) и хранятся на сервере хранилища.
**Ключевые особенности:**

-   Данные обрабатываются перед загрузкой (ETL-процесс).
    
-   Используется строгая схема (таблицы, связи, типы данных).
Используется для аналитики бизнес-отчётности (BI), OLAP-кубов, планирования.
    
3.  **Озёра данных (Data Lakes)** — хранят структурированные, полу- и неструктурированные данные в сыром виде. Оно основано технологиях (Hadoop/HDFS, Spark, облачные хранилища), которые позволяют хранить большие объёмы разнообразных данных для последующего анализа и машинного обучения
**Ключевые особенности:**

-   Нет строгой схемы.
    
-   Поддержка работы с Big Data.
Используется для data science, machine learning, аналитики больших и разнородных данных.
    
5.  **Потоковые платформы (Data Streams)** — системы для непрерывной обработки событий и данных «в движении». Такие платформы (например, Apache Kafka, Flink) позволяют анализировать данные в режиме реального времени, что важно для задач мониторинга, детекции аномалий, обработки финансовых транзакций и др.
**Ключевые особенности:**

-   Постоянная обработка входящего потока.
Используется в задачах мониторинга, биржевых трейдов, систем IoT, систем антифрода.

**Концепция хранилища данных (Data Warehouse)**
Это централизованная система хранения исторических и текущих данных, специально подготовленных для аналитики.

**Признаки:**

 -   Данные интегрируются из разных источников.

 -   Выполняется предварительная обработка (ETL).
    
 -   Строгая структура (схема).
    
 -   Используется для формирования отчётов, дашбордов, аналитики.

**ETL процесс**

**ETL (Extract — Transform — Load)** — это процесс извлечения, преобразования и загрузки данных в хранилище:
**Extract** - извлекает данные из разных источников: БД, CSV, API, логов.

**Transform** - очищает, нормализует, обогащает данные, переводит в нужный формат.

**Load** - загружает обработанные данные в хранилище данных.

**Проблемы интеграции данных из распределенных источников**
В системах, где данные поступают с разных узлов, серверов или облаков, возникает ряд проблем:

 -   **Разный формат данных:** JSON, XML, CSV, SQL, лог-файлы.

 -   **Сложности обновления данных:** проблема устаревания информации часто возникает при работе с big data, особенно если источники обновляются с разной частотой.    

 -   **Угрозы безопасности**: передача данных между системами повышает вероятность утечек и несанкционированного доступа.

 - **Управление качеством данных**: Недостоверная или некорректная информация может свести на нет преимущества интеграции.

 - **Высокие затраты**: Интеграция источников данных требует значительных ресурсов.

**Пример ETL процесса:** 

 - Extract - Чтение CSV и вывод его содержимого
 - Transform - Объединение имени и фамилии, проверка телефонных номеров
 - Load - Создание таблицы, загрузка в неё данных и вывод содержимого

**Код:**

    import  pandas  as  pd
    from  datetime  import  datetime
    import  sqlite3
    
    df_raw  =  pd.read_csv("customers-100.csv")
    print("Исходный CSV:")
    print(df_raw)
    
    def  transform_data(df):
	    df  =  df.copy()
	    # Объединение имени и фамилии в один столбец Full Name
	    df["Full Name"] =  df["First Name"] +  " "  +  df["Last Name"]
    
	    # Оставить только валидные телефоны: убрать null и дефисы
	    for  col  in ["Phone 1", "Phone 2"]:
		    df[col] =  df[col].fillna("Нет данных")
		    df[col] =  df[col].astype(str).str.replace("-", "").str.replace("x", " доб. ")
		    
	    df_clean  =  df[["Customer Id", "Full Name", "Company", "City", "Country", "Phone 1", "Phone 2", "Email", "Subscription Date", "Website"]]
	    
	    return  df_clean

    df_clean  =  transform_data(df_raw)

    print("Измененный CSV:")
    df_clean.to_csv("customers_cleaned.csv", index=False)
    print(df_clean.head())
    
    conn  =  sqlite3.connect("customers.db")
    cursor  =  conn.cursor()
    
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS customers (
	    customer_id TEXT PRIMARY KEY,
	    full_name TEXT,
	    company TEXT,
	    city TEXT,
	    country TEXT,
	    phone1 TEXT,
	    phone2 TEXT,
	    email TEXT,
	    subscription_date TEXT,
	    website TEXT
    )
    """)
    
    # Загрузка данных из DataFrame в SQLite
    df_clean.to_sql("customers", conn, if_exists='replace', index=False)
    
    print("Данные из таблицы SQLite:")
    for  row  in  cursor.execute("SELECT * FROM customers"):
	    print(row)
	    
    conn.close()

**Результат:**

![1](https://github.com/GiveEnd/Distributed-data-analysis/blob/main/1/Result/1.png?raw=true)

![2](https://github.com/GiveEnd/Distributed-data-analysis/blob/main/1/Result/2.png?raw=true)

![3](https://github.com/GiveEnd/Distributed-data-analysis/blob/main/1/Result/3.png?raw=true)


## **Неструктурированные данные. Озера данных.**

**Неструктурированные данные** - это данные без фиксированной схемы хранения, не укладывающиеся в табличный формат.

Примеры: текстовые файлы, изображения, видео и аудио сообщения из соцсетей, логи серверов, документы PDF.

Для хранения и анализа таких данных нужны специальные системы: файловые хранилища, NoSQL, облачные хранилища, озёра данных.

**Полуструктурированные данные** - это данные, которые имеют определённую структуру, но она может быть гибкой и неполной.

Примеры: JSON, XML, YAML, CSV с разным количеством столбцов, лог-файлы с ключами и значениями.

**Концепция «Озёр» данных (Data Lake)**

**Data Lake** — это хранилище сырых данных любых форматов:  
структурированных, полуструктурированных, неструктурированных.

**Основные свойства:**

-   Данные хранятся «как есть», без предварительной обработки.
  
-   Позволяет централизованно собирать большие объёмы данных с разных источников.
    
-   Используется schema-on-read: структура данных определяется во время обработки.

**Средства распределённого анализа данных**
Поскольку объёмы данных в озёрах огромны, анализировать их на одном сервере невозможно.  
Используются **распределённые системы обработки данных**:
-   Apache Hadoop
-   Apache Spark
-   Apache Hive
-   Apache Flink
-   Presto
-   Dask (Python)

Они позволяют обрабатывать данные параллельно на кластере из множества серверов.

**Концепция MapReduce**

**MapReduce** — это модель для распределённой обработки данных.
Состоит из двух этапов:

 - Map - разбивает данные на фрагменты, применяет к каждому свою функцию
 - Reduce - агрегирует промежуточные результаты

**Пример MapReduce: подсчёт слов в больших текстах, распределяя работу по нескольким процессам.** 

**Код:**

    from  multiprocessing  import  Pool
    from  collections  import  Counter
    
    texts  = [
    "data lake big data hadoop spark",
    "spark big data mapreduce data lake",
    "hadoop mapreduce hive big data",
    "big data streaming spark hadoop"
    ]
    
    def  count_words(text):
	    words  =  text.split()
	    return  Counter(words)
	    
    # Инициализация пула из 4 параллельных процессов
    if  __name__  ==  '__main__':
	    with  Pool(4) as  pool:
		    results  =  pool.map(count_words, texts)
		    
    # Reduce-функция: объединение словарей с частотами
    final_counts  =  Counter()
    for  result  in  results:
	    final_counts.update(result)
	        
    print(final_counts)
    
**Результат:**

![1](https://github.com/GiveEnd/Distributed-data-analysis/blob/main/2/Result/1.png?raw=true)

## **Потоковые данные.**

**Потоковые данные** — это данные, которые поступают непрерывным потоком от различных источников в режиме реального времени или близком к реальному времени.

Примеры:

-   торговые операции на бирже
    
-   логи приложений

Проблемы обработки потоковых данных:
 -   Высокая скорость поступления данных — нужно обрабатывать без задержек.
    
 -   Большой объём данных — нельзя всё хранить в оперативной памяти.
    
 -   Непрерывный характер — нет момента «стоп», когда можно обработать весь набор.
    
 -   Данные могут приходить с задержками или в неправильном порядке.
    
 -   Обеспечение отказоустойчивости — потеря пакета может быть критична.

**Свойства систем анализа потоковых данных:**

 - Низкую задержку - мгновенная реакция на событие
 - Моментальная обработка - данные обрабатываются сразу при поступлении
 - Масштабируемость - возможность увеличивать число узлов
 - Отказоустойчивость - автоматическое восстановление после сбоя
 - Упорядоченность - корректная работа даже с данными, пришедшими с задержкой

**Лямбда-архитектура**
Это архитектурный подход для обработки больших потоков и пакетных данных одновременно.

**Состоит из трёх слоёв:**

1.  **Batch-слой** — хранит полный набор данных и периодически пересчитывает агрегаты.
    
2.  **Speed-слой** — быстро обрабатывает только новые данные.
    
3.  **Serving-слой** — объединяет результаты batch и speed для выдачи финальных аналитических отчётов.

**Капа-архитектура**
Это упрощенный вариант Лямбда-архитектуры, который позволяет обрабатывать потоковые и пакетные данные с использованием одного технологического стека, минуя отдельные батч- и скорость-уровни.
 Все данные рассматриваются как поток и обрабатываются с помощью единого потокового процессора.

**Обработка потоковых данных - система онлайн-мониторинга цен на акции.** 

 - Имитация потока поступающих цен акций
 - Хранение последних 5 котировок для каждой акции
 - Вычисление изменения цены в процентах между первой и последней ценой из окна  
 - Вывод алерта, если изменение больше 5%

**Код:**

    import  random
    import  time
    from  collections  import  deque
    
    stocks  = ['YNDX', 'SBER', 'GAZP']
    
    # Очередь для хранения последних 5 цен по каждой акции
    price_history  = {stock: deque(maxlen=5) for  stock  in  stocks}
    
    # Начальные цены
    current_prices  = {stock: random.uniform(100, 200) for  stock  in  stocks}
    
    # Поток новых цен (генерация раз в 0.5 сек)
    def  stream_stock_prices():
	    while  True:
		    stock  =  random.choice(stocks)
		    # Имитация изменения цены на случайное значение
		    change  =  random.uniform(-3, 3)
		    new_price  =  round(current_prices[stock] +  change, 2)
		    current_prices[stock] =  new_price
		    yield  stock, new_price
		    time.sleep(0.5)
		    
    # Обработка потока
    for  stock, price  in  stream_stock_prices():
	    price_history[stock].append(price)
	    print(f"{stock}: новая цена {price} ₽, последние значения: {list(price_history[stock])}")
	    
	    # Если накопилось хотя бы 2 значения — считается динамика
	    if  len(price_history[stock]) >=  2:
		    old_price  =  price_history[stock][0]
		    new_price  =  price_history[stock][-1]
		    delta  = ((new_price  -  old_price) /  old_price) *  100
	    
		    print(f"Изменение по {stock} за последние {len(price_history[stock])} обновлений: {delta:.2f}%")

		    # Если скачок больше чем на 5% — выводится алерт
		    if  abs(delta) >  5:
			    print(f"АЛЕРТ: цена {stock} изменилась на {delta:.2f}% за {len(price_history[stock])} обновлений!\n")

**Результат:**

![1](https://github.com/GiveEnd/Distributed-data-analysis/blob/main/3/Result/1.png?raw=true)

## **Федеративное обучение.**
**Проблемы централизованного анализа данных**

**Централизованный анализ** — это когда все данные со всех компьютеров, телефонов, серверов собираются в одно место (например, на сервер в облаке), и там анализируются.

Проблемы:
 - Огромный объём данных — сложно и дорого передавать и хранить.
 - Риски утечки конфиденциальной информации.
 - Большая задержка при передаче данных на сервер.

**Федеративное обучение**

**Федеративное обучение** - метод машинного обучения, при котором модель обучается совместно на данных, распределённых по многим устройствам/сервером, а обучается модель локально.

Принцип обучения:
 - Сервер рассылает начальную модель устройствам.
 - Каждый клиент (телефон, сервер) обучает эту модель на своих данных.
 - Вместо данных клиенты отправляют только обновлённые параметры модели.
 - Сервер собирает все обновления, усредняет их — получается новая улучшенная модель.

**Виды систем федеративного обучения:**

 - Централизованное - есть главный сервер, который собирает параметры от клиентов
 - Децентрализованное - устройства обмениваются параметрами друг с другом без сервера
 - Гибридные системы - комбинация централизованного и децентрализованного подходов

**Проблемы построения систем федеративного обучения:**

 - Разная производительность устройств.
 - Данные на разных устройствах могут быть разными по объёму и содержанию.
 - Нужно поддерживать синхронность или асинхронность обучения.
 - Сложнее контролировать, не отправляют ли клиенты «вредные» или неправильные данные.
 - Балансировка нагрузки между клиентами.

**Основные алгоритмы федеративного обучения:**

 - FedAvg - клиенты обучают модель на своих данных, отправляют обновления серверу, сервер усредняет параметры.
 - FedProx - Модификация FedAvg с регуляризацией, чтобы уравновесить вклад разных клиентов.
 - FedOpt - Расширение FedAvg с использованием продвинутых оптимизаторов (Adam, Yogi).

**Федеративное обучение линейной регрессии на распределённых данных с использованием Flower и алгоритма FedAvg.** 

 - Данные остаются на клиентах, а сервер управляет обучением, собирает параметры локальных моделей, усредняет их (FedAvg) и рассылает обратно.
 - Модель обучается на синтетических данных с линейной зависимостью вида `y = a * x + b`, с добавлением случайного шума.
 - После каждого раунда сервер выводит историю средней ошибки (loss), показывая, как модель улучшается от раунда к раунду.

**Код:**

> server.py
   
    import  flwr  as  fl
    
    # Запуск сервера
    if  __name__  ==  "__main__":
	    fl.server.start_server(
		    server_address="localhost:8080",
		    config=fl.server.ServerConfig(num_rounds=50)
    )
> client.py

    import  flwr  as  fl
    import  numpy  as  np
    from  sklearn.linear_model  import  SGDRegressor
    
    # Данные клиента
    X  =  np.random.rand(20, 1)
    y  =  3  *  X.squeeze() +  np.random.normal(0, 0.1, 20)
    
    # Модель линейной регрессии
    model  =  SGDRegressor(max_iter=1, learning_rate='constant', eta0=0.01)
    
    # Инициализируется модель одной тренировкой, чтобы появились coef_ и intercept_
    model.partial_fit(np.array([[0.0]]), np.array([0.0]))
    
    # Клиент Flower
    class  FlowerClient(fl.client.NumPyClient):
	    def  get_parameters(self, config):
		    return [model.coef_, np.array([model.intercept_])]
		    
	    def  fit(self, parameters, config):
		    model.coef_  =  parameters[0]
		    model.intercept_  =  parameters[1]
		    model.partial_fit(X, y)
		    return  self.get_parameters(config), len(X), {}
		    
    def  evaluate(self, parameters, config):
	    model.coef_  =  parameters[0]
	    model.intercept_  =  parameters[1]
	    loss  =  np.mean((model.predict(X) -  y) **  2)
	    return  float(loss), len(X), {}
	    
    if  __name__  ==  "__main__":
	    fl.client.start_numpy_client(server_address="localhost:8080", client=FlowerClient())

**Результат:**

![1](https://github.com/GiveEnd/Distributed-data-analysis/blob/main/4/Result/1.png?raw=true)
